Input Files Path : ./benchmarks/FB15K237/
The toolkit is importing datasets.
The total of relations is 36.
The total of entities is 71104.
The total of train triples is 794314.
Input Files Path : ./benchmarks/FB15K237/
The toolkit is importing datasets.
The total of relations is 36.
The total of entities is 71104.
The total of train triples is 794314.
Input Files Path : ./benchmarks/FB15K237/
The toolkit is importing datasets.
The total of relations is 36.
The total of entities is 71104.
The total of train triples is 794314.
`bottleneck` is a tool that can be used as an initial step for debugging
bottlenecks in your program.

It summarizes runs of your script with the Python profiler and PyTorch's
autograd profiler. Because your script will be profiled, please ensure that it
exits in a finite amount of time.

For more complicated uses of the profilers, please see
https://docs.python.org/3/library/profile.html and
https://pytorch.org/docs/master/autograd.html#profiler for more information.
Running environment analysis...
Running your script with cProfile
0
2884771248.421875
1
986287282.0
2
197612633.5
3
189187951.0
4
157235161.75
5
260717126.0
6
96366147.0
7
80943548.0
8
269198795.875
9
93183383.25
10
81364499.625
11
80256417.5
12
54110457.5
13
39684818.8125
14
22294098.6484375
15
30878705.015625
16
36453109.03125
17
9769400.08203125
18
14576653.296875
19
19888149.61328125
20
23562973.98046875
21
15158123.875
22
1246889.21484375
23
13951208.65625
24
13282170.8203125
25
8541323.59765625
26
12612347.7734375
27
6685523.7109375
28
665782.62109375
29
14993053.73046875
30
811516.6015625
31
28978852.90625
32
862606.03515625
33
587231.9765625
34
20271235.71484375
35
606721.0859375
36
482161.35546875
37
437562.33984375
38
445650.83203125
39
20481000.1640625
40
578412.03515625
41
405212.203125
42
423603.38671875
43
18368584.32421875
44
583327.51953125
45
413148.80859375
46
388306.25
47
340700.1484375
48
301563.4375
49
342209.4296875
50
329989.19921875
51
322834.9375
52
316301.75
53
311126.66015625
54
273000.1796875
55
279298.04296875
56
296963.14453125
57
269203.78125
58
276313.2890625
59
266473.2421875
60
251043.1953125
61
261100.28125
62
273013.6875
63
239166.99609375
64
264564.6796875
65
249607.20703125
66
258507.26953125
67
209578.83203125
68
224971.1015625
69
220572.14453125
70
205957.3828125
71
224025.578125
72
220656.6171875
73
209623.80859375
74
210346.62109375
75
212498.9296875
76
217440.68359375
77
204721.94140625
78
190188.7734375
79
216231.65625
80
206858.109375
81
204389.30078125
82
203390.0859375
83
181671.375
84
189803.78125
85
186611.42578125
86
198514.453125
87
205679.5234375
88
161697.3359375
89
195724.96875
90
174708.5625
91
181197.06640625
92
145376.75
93
176166.9453125
94
171416.16796875
95
167272.46484375
96
161220.99609375
97
165787.0625
98
158231.9921875
99
158912.99609375
Running your script with the autograd profiler...
0
2797797243.1132812
1
942973669.0
2
198714140.5
3
183025301.75
4
305581209.25
5
122838485.5
6
111763133.25
7
83685042.75
8
89455665.25
9
51303729.625
10
158200138.09765625
11
91001039.6796875
12
10016655.625
13
12161915.16796875
14
15928947.6328125
15
1092686.673828125
16
15205562.08203125
17
26075140.109375
18
1218802.3046875
19
15503137.755859375
20
1257394.1953125
21
576292.33203125
22
465105.01171875
23
473052.328125
24
461688.0546875
25
441464.41015625
26
398165.078125
27
396193.19921875
28
341581.984375
29
302633.65625
30
308912.59375
31
298553.72265625
32
278958.26171875
33
309128.078125
34
273231.71875
35
281595.328125
36
277856.87890625
37
233610.48828125
38
262467.1796875
39
271631.29296875
40
271491.5234375
41
265839.4140625
42
254003.703125
43
205902.4765625
44
242823.20703125
45
207075.68359375
46
230344.91015625
47
244879.98828125
48
228227.53125
49
230525.62109375
50
237107.2578125
51
195847.69921875
52
202935.53125
53
202133.52734375
54
191048.8515625
55
174670.22265625
56
206230.37109375
57
191172.0234375
58
178246.26171875
59
150412.98046875
60
200690.3515625
61
163965.2109375
62
151802.24609375
63
172028.34375
64
156728.03515625
65
174128.8046875
66
161610.55859375
67
165398.46875
68
159042.7734375
69
143524.86328125
70
166155.0234375
71
154981.73046875
72
156557.765625
73
146833.8203125
74
138161.57421875
75
126658.83984375
76
141549.80078125
77
141446.90234375
78
130582.1796875
79
133463.25390625
80
129859.484375
81
133299.171875
82
126717.83203125
83
126818.57421875
84
135085.30859375
85
133730.4765625
86
122130.80859375
87
112109.91015625
88
125920.3671875
89
115162.94140625
90
124625.0234375
91
124581.55859375
92
125956.37109375
93
117263.0546875
94
119607.3828125
95
117437.86328125
96
111896.88671875
97
113783.3203125
98
105229.92578125
99
106995.0
0
3041457409.109375
1
1015848917.0
2
198998971.5
3
182368649.0
4
180241601.0
5
287629226.5
6
109357843.5
7
91501720.25
8
76600632.1875
9
59920381.125
10
187771307.6953125
11
25167988.296875
12
13632880.1953125
13
14944839.17578125
14
13801806.66796875
15
22588923.01171875
16
2192572.40625
17
880322.9609375
18
14260410.66796875
19
650591.57421875
20
13841592.09375
21
651195.25390625
22
547489.46484375
23
471970.35546875
24
488202.97265625
25
353912.125
26
373306.72265625
27
422445.9375
28
332057.08984375
29
335928.45703125
30
312642.5390625
31
339593.859375
32
291269.6171875
33
298386.2109375
34
273350.03125
35
314953.27734375
36
267814.6015625
37
283756.72265625
38
261982.4609375
39
280172.30859375
40
238483.64453125
41
257939.39453125
42
248153.7421875
43
219786.0
44
213961.15625
45
245678.1953125
46
219855.81640625
47
207621.98046875
48
227575.01171875
49
187044.703125
50
202454.79296875
51
195069.3515625
52
187160.69140625
53
175677.44921875
54
179488.3515625
55
185016.7734375
56
186503.38671875
57
185355.33203125
58
162180.1171875
59
187117.78125
60
176807.11328125
61
168294.125
62
184672.640625
63
159573.16796875
64
151684.51953125
65
146948.1484375
66
160783.9921875
67
180729.10546875
68
158786.7734375
69
148996.50390625
70
135122.3125
71
159169.578125
72
142249.46875
73
154206.28125
74
131129.58203125
75
138586.84765625
76
134224.65625
77
138847.2421875
78
136987.55859375
79
131175.23046875
80
132420.1640625
81
139138.57421875
82
129786.90625
83
122126.1875
84
120875.640625
85
129222.95703125
86
129613.19140625
87
110331.23046875
88
108572.09765625
89
135564.34765625
90
123099.8515625
91
124599.75
92
108063.66015625
93
117491.48046875
94
112338.95703125
95
98184.89453125
96
114088.45703125
97
107228.4765625
98
109272.51171875
99
109828.11328125
--------------------------------------------------------------------------------
  Environment Summary
--------------------------------------------------------------------------------
PyTorch 1.0.0 compiled w/ CUDA 9.0.176
Running with Python 3.6 and CUDA 9.2.148

`pip list` truncated output:
Unable to fetch
--------------------------------------------------------------------------------
  cProfile output
--------------------------------------------------------------------------------
         698691 function calls (678914 primitive calls) in 525.253 seconds

   Ordered by: internal time
   List reduced from 393 to 15 due to restriction <15>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     2800  396.757    0.142  396.757    0.142 {method 'run_backward' of 'torch._C._EngineBase' objects}
     2800   65.482    0.023   65.482    0.023 /MIUN/OpenKE/config/Config.py:228(sampling)
    19602   52.243    0.003   52.243    0.003 {method 'cuda' of 'torch._C._TensorBase' objects}
        1    3.919    3.919    3.919    3.919 /usr/lib/python3.6/json/encoder.py:204(iterencode)
    16800    0.731    0.000    0.731    0.000 {built-in method embedding}
     2800    0.664    0.000    0.664    0.000 {built-in method margin_ranking_loss}
     5600    0.458    0.000    0.658    0.000 /MIUN/OpenKE/models/TransE.py:27(_calc)
        1    0.458    0.458    0.458    0.458 /MIUN/OpenKE/config/Config.py:113(init)
    50449    0.406    0.000    0.599    0.000 /root/.local/share/virtualenvs/kgexpr-i_6t9-hg/lib/python3.6/site-packages/torch/nn/modules/module.py:537(__setattr__)
     5600    0.352    0.000    0.352    0.000 {built-in method mean}
     5598    0.271    0.000    0.271    0.000 {method 'zero_' of 'torch._C._TensorBase' objects}
        2    0.261    0.130    0.261    0.130 {method 'tolist' of 'numpy.ndarray' objects}
22400/2800    0.256    0.000   54.988    0.020 /root/.local/share/virtualenvs/kgexpr-i_6t9-hg/lib/python3.6/site-packages/torch/nn/modules/module.py:483(__call__)
     5600    0.208    0.000    0.208    0.000 {method 'add_' of 'torch._C._TensorBase' objects}
     5600    0.200    0.000    0.200    0.000 {built-in method abs}


--------------------------------------------------------------------------------
  autograd profiler output (CUDA mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

	Because the autograd profiler uses the CUDA event API,
	the CUDA time column reports approximately max(cuda_time, cpu_time).
	Please ignore this output if your code does not use CUDA.

----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                                 CPU time        CUDA time            Calls        CPU total       CUDA total
----------------------------  ---------------  ---------------  ---------------  ---------------  ---------------
EmbeddingBackward                136252.498us      11939.880us                1     136252.498us      11939.880us
embedding_backward               136223.445us      11936.768us                1     136223.445us      11936.768us
embedding_dense_backward         136208.220us      11928.589us                1     136208.220us      11928.589us
EmbeddingBackward                123089.685us      10353.699us                1     123089.685us      10353.699us
embedding_backward               123080.605us      10350.586us                1     123080.605us      10350.586us
embedding_dense_backward         123071.317us      10344.482us                1     123071.317us      10344.482us
EmbeddingBackward                120374.660us      10031.250us                1     120374.660us      10031.250us
embedding_backward               120366.278us      10031.250us                1     120366.278us      10031.250us
embedding_dense_backward         120354.126us      10000.000us                1     120354.126us      10000.000us
EmbeddingBackward                119798.272us       9781.250us                1     119798.272us       9781.250us
embedding_backward               119790.241us       9750.000us                1     119790.241us       9750.000us
embedding_dense_backward         119778.926us       9750.000us                1     119778.926us       9750.000us
EmbeddingBackward                119504.534us      10015.625us                1     119504.534us      10015.625us
embedding_backward               119495.664us      10015.625us                1     119495.664us      10015.625us
embedding_dense_backward         119482.325us      10007.812us                1     119482.325us      10007.812us

